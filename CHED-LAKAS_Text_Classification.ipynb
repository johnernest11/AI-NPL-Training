{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5c72a2",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "This cell code is a summarization of the previous text preprocessing activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1c73d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jagoodkid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/var/folders/ln/68xn7sb51jjbm1hj8k0rslsr0000gn/T/ipykernel_27453/2656058787.py:8: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('data_supervised.csv', error_bad_lines= False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') \n",
    "#File path\n",
    "df = pd.read_csv('data_supervised.csv', error_bad_lines= False)\n",
    "#drop null values\n",
    "df.dropna(inplace=True)\n",
    "filename = 'english_words.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    stop_words = file.read().splitlines()\n",
    "filename = 'tagalog_stop_words.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    tagalog_words = file.read().splitlines()\n",
    "stop_words.extend(tagalog_words)\n",
    "def preprocess_data(article):\n",
    "    stopwords=stop_words\n",
    "    article = str(article).lower()\n",
    "    article = re.sub(\"[^a-zA-Z0-9\\s]\",'',article)\n",
    "    temp_final =[]\n",
    "    for word in article.split():\n",
    "        if word =='' or '\\r\\n' in word or word in stop_words:\n",
    "            None\n",
    "        else:\n",
    "            temp_final.append(word)\n",
    "    return word_tokenize(' '.join(temp_final))\n",
    "df['Article_processed'] = df['Article'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656d90db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Category</th>\n",
       "      <th>Article_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MANILA, Philippines - ï¿½Bad morningï¿½ ang su...</td>\n",
       "      <td>Metro</td>\n",
       "      <td>[manila, philippines, bad, morning, sumalubong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kabilang sa mga bagong panuntunan saï¿½ Operat...</td>\n",
       "      <td>Bansa</td>\n",
       "      <td>[kabilang, bagong, panuntunan, operational, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBINULONG nang mga asset ng mga kuwago ng ORA ...</td>\n",
       "      <td>Opinyon</td>\n",
       "      <td>[ibinulong, nang, asset, kuwago, ora, private,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MANILA, Philippines - Dalawa ang iniulat na na...</td>\n",
       "      <td>Metro</td>\n",
       "      <td>[manila, philippines, iniulat, nasawi, pasay, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biruin mo, nagamit pa rin niï¿½Angelika dela C...</td>\n",
       "      <td>Showbiz</td>\n",
       "      <td>[biruin, mo, nagamit, rin, niangelika, dela, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article Category  \\\n",
       "0  MANILA, Philippines - ï¿½Bad morningï¿½ ang su...    Metro   \n",
       "1  Kabilang sa mga bagong panuntunan saï¿½ Operat...    Bansa   \n",
       "2  IBINULONG nang mga asset ng mga kuwago ng ORA ...  Opinyon   \n",
       "3  MANILA, Philippines - Dalawa ang iniulat na na...    Metro   \n",
       "4  Biruin mo, nagamit pa rin niï¿½Angelika dela C...  Showbiz   \n",
       "\n",
       "                                   Article_processed  \n",
       "0  [manila, philippines, bad, morning, sumalubong...  \n",
       "1  [kabilang, bagong, panuntunan, operational, pr...  \n",
       "2  [ibinulong, nang, asset, kuwago, ora, private,...  \n",
       "3  [manila, philippines, iniulat, nasawi, pasay, ...  \n",
       "4  [biruin, mo, nagamit, rin, niangelika, dela, c...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check first 5 instances\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9437f0",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning\n",
    "In this activity, you will step into the shoes of a data scientist and work on building a Filipino news classifier using the power of Support Vector Machines (SVM). The goal is to create a program that can automatically categorize Filipino news articles into different topics, such as politics, entertainment, sports, and more. This will help you understand how SVM works in text classification tasks and how it can be applied to real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93495ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa66de3",
   "metadata": {},
   "source": [
    "#### We just imported CountVectorizer\n",
    "Count Vectorizer is a fundamental technique used in natural language processing (NLP) to convert text data into numerical form that machine learning algorithms, like Support Vector Machines (SVM), can understand and work with. It's a way to represent text as a collection of word counts. Let's break it down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a27be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a CountVectorizer model\n",
    "df['Article'] = df['Article_processed']\n",
    "bow_transformer= CountVectorizer(analyzer=preprocess_data).fit(df['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657a937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66779\n"
     ]
    }
   ],
   "source": [
    "#Print number of vocabulary/words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4942652",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#trying the countvectorizer for a single instance\n",
    "artc3=df['Article'][2]\n",
    "bow3=bow_transformer.transform([artc3])\n",
    "print(bow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba3da3",
   "metadata": {},
   "source": [
    "\n",
    "### 1.  Now try it with the whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71874cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter code here, name it as article_bow\n",
    "article_bow=bow_transformer.transform(df['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767dbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#Making an instance of this transformer\n",
    "tfidf_transformer=TfidfTransformer().fit(article_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52c2e9",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "stands for Term Frequency-Inverse Document Frequency. It's a technique used in natural language processing (NLP) to help understand the importance of words in a collection of documents. Let's break it down:\n",
    "\n",
    "#### Term Frequency (TF):\n",
    "Imagine you have a document (like an article or a book). Term Frequency is the number of times a specific word appears in that document. It helps us know which words are important in that particular document.\n",
    "\n",
    "#### Inverse Document Frequency (IDF):\n",
    "Now, think about all the documents you have in your collection. Inverse Document Frequency measures how unique or rare a word is across all those documents. It tells us which words are special and not common in the entire collection.\n",
    "\n",
    "#### Combining TF and IDF - TF-IDF:\n",
    "TF-IDF combines Term Frequency and Inverse Document Frequency. It helps us understand how significant a word is in a particular document compared to its significance in the entire collection. If a word appears a lot in a document but is rare in the whole collection, its TF-IDF value will be high for that document.\n",
    "\n",
    "#### Why Use TF-IDF?\n",
    "Imagine you're analyzing a bunch of articles about cats. The word \"cat\" might appear a lot in all of them, but words like \"purr\" or \"kitten\" might appear less frequently. TF-IDF helps us identify these less common, more interesting words that give a document its unique character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc97e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to get what it looks like for a single article\n",
    "tfidf3=tfidf_transformer.transform(bow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb437589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 63852)\t0.11406685891971156\n",
      "  (0, 62167)\t0.09819456281998327\n",
      "  (0, 61511)\t0.08554337049213658\n",
      "  (0, 60487)\t0.1089296841980396\n",
      "  (0, 57455)\t0.0903514401700809\n",
      "  (0, 57133)\t0.0781705223888095\n",
      "  (0, 57055)\t0.0593650107102201\n",
      "  (0, 56174)\t0.04748831424141843\n",
      "  (0, 55960)\t0.07975631835795811\n",
      "  (0, 55079)\t0.45626743567884626\n",
      "  (0, 54342)\t0.09458273977041391\n",
      "  (0, 53951)\t0.07184838824295289\n",
      "  (0, 53651)\t0.11406685891971156\n",
      "  (0, 53537)\t0.11406685891971156\n",
      "  (0, 53533)\t0.08489349307963008\n",
      "  (0, 53090)\t0.06208705753744363\n",
      "  (0, 52182)\t0.1089296841980396\n",
      "  (0, 50499)\t0.0707648366900736\n",
      "  (0, 49147)\t0.1089296841980396\n",
      "  (0, 46001)\t0.1089296841980396\n",
      "  (0, 45922)\t0.2822352745024029\n",
      "  (0, 45868)\t0.07461914363628616\n",
      "  (0, 45862)\t0.05356951483577385\n",
      "  (0, 45439)\t0.10245760946431699\n",
      "  (0, 43361)\t0.08206630181657898\n",
      "  :\t:\n",
      "  (0, 21540)\t0.06656127293341496\n",
      "  (0, 21468)\t0.10245760946431699\n",
      "  (0, 20824)\t0.09246798732474933\n",
      "  (0, 20419)\t0.11208179921665491\n",
      "  (0, 19754)\t0.0593650107102201\n",
      "  (0, 19054)\t0.07354020852791156\n",
      "  (0, 18955)\t0.10528480072736807\n",
      "  (0, 17617)\t0.06475815033556809\n",
      "  (0, 17610)\t0.11406685891971156\n",
      "  (0, 17241)\t0.08554337049213658\n",
      "  (0, 14872)\t0.08312273277729129\n",
      "  (0, 13001)\t0.07015656795799423\n",
      "  (0, 11828)\t0.07579858363899261\n",
      "  (0, 11441)\t0.09819456281998327\n",
      "  (0, 9522)\t0.04074784243719797\n",
      "  (0, 6628)\t0.04547396430794756\n",
      "  (0, 6576)\t0.027776516975690193\n",
      "  (0, 6474)\t0.054455103639178846\n",
      "  (0, 6115)\t0.26995499099915704\n",
      "  (0, 6078)\t0.1651670192420184\n",
      "  (0, 5883)\t0.060699910722270746\n",
      "  (0, 5874)\t0.11406685891971156\n",
      "  (0, 4670)\t0.1709545022942475\n",
      "  (0, 4048)\t0.045333654659125006\n",
      "  (0, 3902)\t0.10014762600569614\n"
     ]
    }
   ],
   "source": [
    "#transforming a simple word count into a tfidf \n",
    "print(tfidf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03268623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.931136870164136"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the tfidf of a particular word('Bansa')\n",
    "tfidf_transformer.idf_[bow_transformer.vocabulary_['bansa']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb012253",
   "metadata": {},
   "source": [
    "### Now try to check the tfidf of word \"manila\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5952acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5158549721709926"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter code here\n",
    "tfidf_transformer.idf_[bow_transformer.vocabulary_['manila']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "231db998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the entire bag of words corpus into a tfidf corpus at once\n",
    "article_tfidf=tfidf_transformer.transform(article_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0a6c9",
   "metadata": {},
   "source": [
    "## Training dataset\n",
    "Training a dataset is a crucial step in building machine learning models like the Filipino news classifier using Support Vector Machines (SVM). Training helps the model learn and understand the patterns in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd1939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e1a8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_train,article_test,category_train,category_test=train_test_split(df['Article'],df['Category'],test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1434396",
   "metadata": {},
   "source": [
    "### 3. Now, try to make an instance where the test size is 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c642936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your answer here\n",
    "article_train,article_test,category_train,category_test=train_test_split(df['Article'],df['Category'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff48a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import SVM\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f27ca",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "SVM figures out the best way to draw separator lines based on the numerical fingerprints of the articles. It's like finding the patterns that separate politics from entertainment and sports from technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "affc34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c859fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#think of the pipeline as the steps or methods on a task\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(analyzer=preprocess_data)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05620f",
   "metadata": {},
   "source": [
    "### loss: [‘hinge’, ‘log_loss’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’, ‘squared_epsilon_insensitive’]\n",
    "### penalty: l1,l2,  elasticnet\n",
    "### alpha: float below 0\n",
    "### random state: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f5c29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=text_clf_svm.fit(df['Article'],df['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98fb99",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#predict the test article\n",
    "prediction=text_clf_svm.predict(article_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b3beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report\n",
    "report = classification_report(category_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba65f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Bansa       0.95      0.88      0.91       310\n",
      "       Metro       0.91      0.97      0.94       290\n",
      "     Opinyon       0.99      0.97      0.98       294\n",
      "      Palaro       0.99      1.00      0.99       316\n",
      "  Probinsiya       0.96      0.96      0.96       305\n",
      "     Showbiz       0.99      0.99      0.99       279\n",
      "\n",
      "    accuracy                           0.96      1794\n",
      "   macro avg       0.96      0.96      0.96      1794\n",
      "weighted avg       0.96      0.96      0.96      1794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3d94c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Palaro'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_svm.predict([\"James yap, 54 points sa latest na laro ng gilas pilipinas\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829c25b",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "### 1. article_bow=bow_transformer.transform(df['Article'])\n",
    "### 2. tfidf_transformer.idf_[bow_transformer.vocabulary_['manila']]\n",
    "### 3. article_train,article_test,category_train,category_test=train_test_split(df['Article'],df['Category'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9aee5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
